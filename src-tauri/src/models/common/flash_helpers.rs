//! Flash Attention helpers
//!
//! Ð£Ð½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ helper Ð´Ð»Ñ Flash Attention ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð² Ð»ÑŽÐ±Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ….
//! ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÑ‚ Ð¼ÐµÐ¶Ð´Ñƒ Flash Attention (ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾) Ð¸ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¼ attention.

use candle::{Result, Tensor};

/// ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ scaled dot-product attention Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ð²Ñ‹Ð±Ð¾Ñ€Ð¾Ð¼ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸
///
/// # Arguments
/// * `q` - Query Ñ‚ÐµÐ½Ð·Ð¾Ñ€ [batch, num_heads, seq_len, head_dim]
/// * `k` - Key Ñ‚ÐµÐ½Ð·Ð¾Ñ€ [batch, num_heads, seq_len, head_dim]
/// * `v` - Value Ñ‚ÐµÐ½Ð·Ð¾Ñ€ [batch, num_heads, seq_len, head_dim]
/// * `scale` - ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ð¹ ÐºÐ¾ÑÑ„Ñ„Ð¸Ñ†Ð¸ÐµÐ½Ñ‚ (Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ 1/sqrt(head_dim))
/// * `causal` - Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ causal masking (Ð´Ð»Ñ autoregressive Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸)
///
/// # Returns
/// Output Ñ‚ÐµÐ½Ð·Ð¾Ñ€ [batch, num_heads, seq_len, head_dim]
#[allow(unused_variables)]
pub fn scaled_dot_product_attention(
    q: &Tensor,
    k: &Tensor,
    v: &Tensor,
    scale: f32,
    causal: bool,
) -> Result<Tensor> {
    #[cfg(feature = "flash-attn")]
    {
        // Flash Attention Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ [batch, seq_len, num_heads, head_dim]
        // Ð£ Ð½Ð°Ñ [batch, num_heads, seq_len, head_dim], Ð½ÑƒÐ¶Ð½Ð¾ transpose
        let q_fa = q.transpose(1, 2)?.contiguous()?;
        let k_fa = k.transpose(1, 2)?.contiguous()?;
        let v_fa = v.transpose(1, 2)?.contiguous()?;

        // Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ Flash Attention
        log::debug!("ðŸ”¥ Using Flash Attention: shape={:?}", q_fa.shape());
        let output = candle_flash_attn::flash_attn(&q_fa, &k_fa, &v_fa, scale, causal)?;

        // Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ [batch, num_heads, seq_len, head_dim]
        output.transpose(1, 2)
    }
    #[cfg(not(feature = "flash-attn"))]
    {
        // Fallback Ðº ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ð¾Ð¼Ñƒ attention
        log::debug!("Using standard attention");
        standard_attention(q, k, v, scale, causal)
    }
}

/// Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ð°Ñ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ scaled dot-product attention (fallback)
///
/// Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÐºÐ¾Ð³Ð´Ð° Flash Attention Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾
#[allow(dead_code)]
fn standard_attention(
    q: &Tensor,
    k: &Tensor,
    v: &Tensor,
    scale: f32,
    causal: bool,
) -> Result<Tensor> {
    // q, k, v shape: [batch, num_heads, seq_len, head_dim]

    // 1. Q @ K^T
    let scores = q.matmul(&k.transpose(2, 3)?)?;

    // 2. Scale
    let scores = (scores * scale as f64)?;

    // 3. Causal masking (ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾)
    let scores = if causal {
        let (_, _, seq_len, _) = scores.dims4()?;
        let mask = create_causal_mask(seq_len, scores.device(), scores.dtype())?;
        scores.broadcast_add(&mask)?
    } else {
        scores
    };

    // 4. Softmax
    let probs = candle_nn::ops::softmax_last_dim(&scores)?;

    // 5. @ V
    probs.matmul(v)
}

/// Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ñ‚ causal mask Ð´Ð»Ñ autoregressive Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸
fn create_causal_mask(
    seq_len: usize,
    device: &candle::Device,
    dtype: candle::DType,
) -> Result<Tensor> {
    let mask: Vec<_> = (0..seq_len)
        .flat_map(|i| (0..seq_len).map(move |j| if j > i { f32::NEG_INFINITY } else { 0f32 }))
        .collect();

    Tensor::from_vec(mask, (seq_len, seq_len), device)?
        .to_dtype(dtype)?
        .unsqueeze(0)?
        .unsqueeze(0) // [1, 1, seq_len, seq_len] Ð´Ð»Ñ broadcasting
}

/// ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚, Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ Ð»Ð¸ Flash Attention Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸
pub fn is_flash_attention_available() -> bool {
    #[cfg(feature = "flash-attn")]
    {
        candle::utils::cuda_is_available()
    }
    #[cfg(not(feature = "flash-attn"))]
    {
        false
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use candle::{DType, Device};

    #[test]
    fn test_standard_attention() {
        let device = Device::Cpu;
        let dtype = DType::F32;

        // [batch=1, heads=2, seq=4, dim=8]
        let q = Tensor::randn(0f32, 1f32, (1, 2, 4, 8), &device)
            .unwrap()
            .to_dtype(dtype)
            .unwrap();
        let k = Tensor::randn(0f32, 1f32, (1, 2, 4, 8), &device)
            .unwrap()
            .to_dtype(dtype)
            .unwrap();
        let v = Tensor::randn(0f32, 1f32, (1, 2, 4, 8), &device)
            .unwrap()
            .to_dtype(dtype)
            .unwrap();

        let output = scaled_dot_product_attention(&q, &k, &v, 1.0 / (8f32).sqrt(), true).unwrap();

        assert_eq!(output.dims(), &[1, 2, 4, 8]);
    }
}
