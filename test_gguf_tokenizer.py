#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≤ GGUF —Ñ–∞–π–ª–∞—Ö.
–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø–æ–º–æ–∂–µ—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏ –≤ GGUF —Ñ–∞–π–ª–∞—Ö.
"""

import sys
import json
from pathlib import Path

def check_gguf_tokenizer(file_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≤ GGUF —Ñ–∞–π–ª–µ."""
    print(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞: {file_path}")
    
    try:
        # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GGUF
        try:
            import gguf
        except ImportError:
            print("‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ gguf –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ—ë –∫–æ–º–∞–Ω–¥–æ–π:")
            print("   pip install gguf")
            return False
        
        # –ß–∏—Ç–∞–µ–º GGUF —Ñ–∞–π–ª
        reader = gguf.GGUFReader(file_path, "r")
        
        print(f"‚úÖ GGUF —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω")
        print(f"   –í–µ—Ä—Å–∏—è: {reader.version}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(reader.metadata)}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–Ω–∑–æ—Ä–æ–≤: {len(reader.tensors)}")
        
        # –ò—â–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        tokenizer_keys = [
            "tokenizer.json",
            "general.tokenizer_json", 
            "qwen3.tokenizer_json",
            "llama.tokenizer_json",
            "gemma.tokenizer_json",
            "tokenizer.ggml.json",
            "tokenizer_json",
            "tokenizer",
            "tokenizer.ggml.tokenizer_json",
            "tokenizer.model"
        ]
        
        found_tokenizer = False
        for key in tokenizer_keys:
            if key in reader.metadata:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø–æ –∫–ª—é—á—É: {key}")
                found_tokenizer = True
                break
        
        if not found_tokenizer:
            print("‚ùå –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
            
            # –ü—Ä–æ–≤–µ—Ä–∏–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
            token_keys = [
                "tokenizer.ggml.tokens",
                "tokenizer.tokens", 
                "tokenizer.vocab",
                "tokenizer.ggml.vocab",
                "vocab",
                "tokens"
            ]
            
            found_tokens = False
            for key in token_keys:
                if key in reader.metadata:
                    tokens = reader.metadata[key]
                    if hasattr(tokens, '__len__'):
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã —Ç–æ–∫–µ–Ω—ã –ø–æ –∫–ª—é—á—É: {key} (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(tokens)})")
                        found_tokens = True
                        break
            
            if not found_tokens:
                print("‚ùå –¢–æ–∫–µ–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
                
                # –ü–æ–∫–∞–∂–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                print("\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö:")
                for key in sorted(reader.metadata.keys()):
                    if "token" in key.lower() or "vocab" in key.lower():
                        print(f"   üîç {key}")
        
        # –ü—Ä–æ–≤–µ—Ä–∏–º –Ω–∞–ª–∏—á–∏–µ BPE merges
        merge_keys = [
            "tokenizer.ggml.merges",
            "tokenizer.ggml.bpe_merges", 
            "tokenizer.merges",
            "merges",
            "bpe_merges"
        ]
        
        found_merges = False
        for key in merge_keys:
            if key in reader.metadata:
                merges = reader.metadata[key]
                if hasattr(merges, '__len__'):
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã BPE merges –ø–æ –∫–ª—é—á—É: {key} (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(merges)})")
                    found_merges = True
                    break
        
        if not found_merges:
            print("‚ÑπÔ∏è  BPE merges –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (—ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ç–∏–ø–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤)")
        
        return found_tokenizer or found_tokens
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ GGUF —Ñ–∞–π–ª–∞: {e}")
        return False

def main():
    if len(sys.argv) != 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python test_gguf_tokenizer.py <–ø—É—Ç—å_–∫_gguf_—Ñ–∞–π–ª—É>")
        sys.exit(1)
    
    file_path = Path(sys.argv[1])
    
    if not file_path.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        sys.exit(1)
    
    if not file_path.suffix.lower() == '.gguf':
        print(f"‚ùå –§–∞–π–ª –Ω–µ —è–≤–ª—è–µ—Ç—Å—è GGUF —Ñ–∞–π–ª–æ–º: {file_path}")
        sys.exit(1)
    
    success = check_gguf_tokenizer(file_path)
    
    if success:
        print("\n‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞–π–¥–µ–Ω –≤ GGUF —Ñ–∞–π–ª–µ!")
    else:
        print("\n‚ùå –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ GGUF —Ñ–∞–π–ª–µ!")
        print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("   1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ GGUF —Ñ–∞–π–ª –±—ã–ª —Å–æ–∑–¥–∞–Ω —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º")
        print("   2. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é llama.cpp")
        print("   3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏")

if __name__ == "__main__":
    main()
