## Oxide Lab — Техническое описание

Этот документ предназначен для разработчиков и продвинутых пользователей. Содержит точное описание текущего функционала и внутренних механизмов приложения.

## Oxide Lab

Локальный десктоп‑чат с LLM на Tauri 2 + SvelteKit 5 и бэкендом на Rust (Candle). Приложение загружает квантованные веса модели в формате GGUF, стримит ответы токен‑за‑токен и отображает их с форматированием Markdown, кодом и свернутыми «размышлениями».

### Что реально работает сейчас
- Загрузка модели Qwen3 в формате GGUF + отдельный `tokenizer.json` через диалоги выбора файлов.
- Выбор устройства автоматически (CUDA при наличии, иначе CPU) с автоматическим откатом на CPU при OOM.
- Потоковая генерация в UI: ответ «печатается» по мере расчёта, с буферизацией и эвентом `token` из Rust.
- Управление выборкой: температура, Top‑K, Top‑P, Min‑P, repeat penalty (включаются по флагам «Пользовательские параметры»).
- Настройка длины контекста при загрузке модели.
- Управление режимом «размышлений»: префиксы `/think` и `/no_think` для последнего пользовательского сообщения. В ответе блок `<think>...</think>` отображается как сворачиваемая секция «Рассуждения».
- Выгрузка модели из памяти (полная очистка состояния в Rust).
- Кастомная оконная рамка (без нативных декораций): кнопки свернуть/развернуть/закрыть.

### Архитектура
- **Фронтенд (SvelteKit 5, SPA)**
  - Корень UI: `src/lib/chat/Chat.svelte` и `src/routes/+page.svelte`.
  - Компоненты: загрузка/выгрузка (`LoaderPanel.svelte`), композиция сообщения (`Composer.svelte`), список сообщений (`MessageList.svelte`), параметры инференса (`InferenceParams.svelte`).
  - Поток: события `token` → инкрементальный парсер `src/lib/chat/parser/*` → отрисовка сегментов `src/lib/chat/stream/*`.
  - Безопасный Markdown: `marked` + `highlight.js` + очистка через `dompurify`.
  - Диалоги/сообщения: `@tauri-apps/plugin-dialog`.
  - Иконки: `phosphor-svelte`.

- **Бэкенд (Tauri 2, Rust + Candle)**
  - Точка входа: `src-tauri/src/main.rs` → `llm_chat_lib::run()`.
  - Команды Tauri: 
    - `load_model(LoadRequest)` — загрузка GGUF и токенизатора, выбор устройства, настройка длины контекста, опционально `n_gpu_layers`.
    - `unload_model()` — полная очистка состояния модели/токенизатора.
    - `generate_stream(GenerateRequest)` — генерация с потоком событий `token`.
    - `cancel_generation()` — установка флага отмены генерации.
    - `set_device(DevicePreference)` — явный выбор устройства (Auto/Cpu/Cuda/Metal).
    - `is_model_loaded()` — проверка статуса загрузки.
  - Состояние: `src-tauri/src/state.rs` хранит модель, токенизатор, устройство, длину контекста и пр.
  - Модель: минимально вендоренная Qwen3‑совместимая реализация с послойным offload `src-tauri/src/model/qwen3_offload.rs` поверх Candle (GGUF‑веса).
  - Стриминг: `src-tauri/src/generate/stream.rs` подготавливает выборку (Sampling ArgMax/TopK/TopP/TopKThenTopP, Min‑P, repeat penalty), кодирует промпт через `tokenizers`, эмитит чанки в UI каждые ~16ms.

### Поток и специальная разметка
- Поддержка спец‑тегов в выходе модели:
  - `<think>...</think>` — блок «Рассуждения» (выводится свернутым details в UI).
  - `<|im_start|>role`, `<|im_end|>`, `<|user|>`, `<|assistant|>`, `<|system|>` — заголовки/служебные токены чатов.
  - Кодовые блоки: `<|code|>...</|endcode|>`, `<|python|>...</|/python|>`.
  - Медиа: `<|image|>src<|/image|>`, `<|audio|>src<|/audio|>`, `<|video|>src<|/video|>` (рендерятся как соответствующие HTML‑элементы).

### Безопасность рендера
- Все HTML‑фрагменты от модели экранируются, Markdown очищается через DOMPurify с разрешенным набором тегов/атрибутов.
- Потоковый парсер защищен от разрыва UTF‑16 суррогатных пар на границе чанков.

### Поддерживаемые модели
- Квантованные веса в формате GGUF с метаданными Qwen3 (ключи типа `qwen3.*`).
- Обязателен совместимый `tokenizer.json`.

### Горячие действия
- Enter — отправить сообщение.
- Shift+Enter — перенос строки.
- Кнопка «Стоп» появляется на время генерации.

### Как запустить (для разработки)
1) Требования:
   - Node.js, npm
   - Rust (stable) и toolchain для вашей платформы
   - Для CUDA: совместимый драйвер и CUDA‑окружение (не обязательно; по умолчанию CPU)

2) Установка зависимостей:
```bash
npm i
```

3) Режим разработки (запустит Vite и Tauri):
```bash
npm run tauri dev
```

4) Сборка релиза:
```bash
npm run tauri build
```

5) Первый запуск в приложении:
- Нажмите «Выбрать файл модели» и укажите `.gguf`.
- Нажмите «Выбрать файл токенизатора» и укажите `tokenizer.json`.
- При необходимости отрегулируйте «Слои на GPU» и «Длину контекста».
- Нажмите «Загрузить». После статусов «Загрузка модели/токенизатора» появится отметка «Готово».
- Введите сообщение и отправьте. Ответ появится потоково.

### Ограничения и примечания
- Веса ожидаются Qwen3‑совместимые (метаданные `qwen3.*`). Другие GGUF‑модели работать не будут.
- `device: Auto` выберет CUDA при доступности; при OOM часть весов/слоёв переводится на CPU.
- По умолчанию потоковая выборка использует аккуратные дефолты (temperature 0.7, Top‑P 0.9, Top‑K 20, лёгкий repeat penalty) — меняются только при включении «Пользовательских параметров».
- Системный промпт не добавляется — используется ровно история чата и управляющие теги `/think` или `/no_think`.

### Технологии
- [Tauri 2](https://v2.tauri.app)
- [SvelteKit 5](https://kit.svelte.dev)
- [Candle](https://github.com/huggingface/candle)
- [marked](https://marked.js.org) + [highlight.js](https://highlightjs.org)
- [phosphor‑svelte](https://phosphoricons.com)

### Лицензия
MIT. См. файл `LICENSE`.


